{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063195f8-7fd3-4bba-98fa-59a4c9ba0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    random.seed(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 예시\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44191105-cd8d-4716-be6d-eb108d01760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (1) 라이브러리 임포트 및 기본 설정\n",
    "######################################################\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e03ba8-8e4b-41c6-9186-4bb982dbb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (2) 하이퍼파라미터 & 경로 설정\n",
    "######################################################\n",
    "batch_size = 128\n",
    "epochs = 20            # 예시로 20 epoch\n",
    "learning_rate = 0.001\n",
    "num_classes = 300\n",
    "\n",
    "# 데이터셋 경로 (사용 환경에 맞게 수정)\n",
    "train_path = \"./dogun_convnext_tiny_try111/dl-bootcamp-2025-challenge_aug/train\"\n",
    "test_path  = \"/test/final_exam/challenge/test\"\n",
    "\n",
    "# 체크포인트 저장 폴더\n",
    "checkpoint_dir = \"./checkpoint_efficientnet_b3\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)  # 없으면 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb31c47-a451-435e-bcc0-be0b4cad4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   89190\n",
      "Validation samples: 4695\n",
      "Test samples:       4178\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (3) 데이터 변환 & 데이터셋/로더 정의\n",
    "######################################################\n",
    "# 논문 권장: EfficientNet-B3 → 300×300\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Train / Test 데이터셋\n",
    "full_train_dataset = datasets.ImageFolder(train_path, transform=transform_train)\n",
    "test_dataset       = datasets.ImageFolder(test_path,  transform=transform_test)\n",
    "\n",
    "# Validation Split (예: 5%)\n",
    "train_size = int(0.95 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader      = DataLoader(train_dataset,      batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader       = DataLoader(test_dataset,       batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(validation_dataset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a88ef7b0-2ecf-4a77-bba4-ef002e3f023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (4) 모델 정의 (EfficientNet-B3, scratch) & (선택)체크포인트 로드\n",
    "######################################################\n",
    "# 처음부터(scratch) → weights=None\n",
    "effnet_b3 = models.efficientnet_b3(weights=None)\n",
    "\n",
    "# 분류기 부분 수정 → 출력 차원 = 300개\n",
    "in_features = effnet_b3.classifier[1].in_features  # 일반적으로 1536\n",
    "effnet_b3.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "model = effnet_b3.to(device)\n",
    "\n",
    "# [원하는 체크포인트 로드 시 주석 해제 예시]\n",
    "# ckpt_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_14.pth\")\n",
    "# model.load_state_dict(torch.load(ckpt_path))\n",
    "# print(f\"Loaded checkpoint: {ckpt_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03570a70-ffb5-41af-8617-20f42a89ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (5) 옵티마이저, 스케줄러, 손실함수 정의\n",
    "######################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13965c60-9ba9-4c2b-8624-c50afc3bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (6) 학습 함수, 검증 함수, 테스트 함수\n",
    "######################################################\n",
    "def train_model(num_epochs=epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss  = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(f\"  [Batch {batch_idx+1}/{total_batches}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation Loss\n",
    "        val_loss = validate_model()\n",
    "\n",
    "        # 매 Epoch마다 체크포인트 저장\n",
    "        ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "        # 스케줄러 step\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"==> Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {running_loss/total_batches:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "\n",
    "def validate_model():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(validation_loader)\n",
    "\n",
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    all_preds  = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total   += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # F1, 혼동행렬 등을 위해 저장\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32bf50-718f-40d5-8aa7-b4a9a6ec48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  [Batch 10/697] Loss: 5.7658\n",
      "  [Batch 20/697] Loss: 5.7318\n",
      "  [Batch 30/697] Loss: 5.5755\n",
      "  [Batch 40/697] Loss: 5.5357\n",
      "  [Batch 50/697] Loss: 5.4891\n",
      "  [Batch 60/697] Loss: 5.1448\n",
      "  [Batch 70/697] Loss: 5.1922\n",
      "  [Batch 80/697] Loss: 5.2258\n",
      "  [Batch 90/697] Loss: 4.9997\n",
      "  [Batch 100/697] Loss: 4.9275\n",
      "  [Batch 110/697] Loss: 5.0622\n",
      "  [Batch 120/697] Loss: 4.8077\n",
      "  [Batch 130/697] Loss: 4.8134\n",
      "  [Batch 140/697] Loss: 4.7954\n",
      "  [Batch 150/697] Loss: 4.7897\n",
      "  [Batch 160/697] Loss: 4.7874\n",
      "  [Batch 170/697] Loss: 4.6618\n",
      "  [Batch 180/697] Loss: 4.7769\n",
      "  [Batch 190/697] Loss: 4.7537\n",
      "  [Batch 200/697] Loss: 4.6750\n",
      "  [Batch 210/697] Loss: 4.7512\n",
      "  [Batch 220/697] Loss: 4.6122\n",
      "  [Batch 230/697] Loss: 4.6075\n",
      "  [Batch 240/697] Loss: 4.7130\n",
      "  [Batch 250/697] Loss: 4.6648\n",
      "  [Batch 260/697] Loss: 4.5440\n",
      "  [Batch 270/697] Loss: 4.5439\n",
      "  [Batch 280/697] Loss: 4.5187\n",
      "  [Batch 290/697] Loss: 4.5154\n",
      "  [Batch 300/697] Loss: 4.5043\n",
      "  [Batch 310/697] Loss: 4.3692\n",
      "  [Batch 320/697] Loss: 4.3583\n",
      "  [Batch 330/697] Loss: 4.4042\n",
      "  [Batch 340/697] Loss: 4.4248\n",
      "  [Batch 350/697] Loss: 4.4284\n",
      "  [Batch 360/697] Loss: 4.2873\n",
      "  [Batch 370/697] Loss: 4.4002\n",
      "  [Batch 380/697] Loss: 4.3249\n",
      "  [Batch 390/697] Loss: 4.4536\n",
      "  [Batch 400/697] Loss: 4.4295\n",
      "  [Batch 410/697] Loss: 4.3412\n",
      "  [Batch 420/697] Loss: 4.3952\n",
      "  [Batch 430/697] Loss: 4.2700\n",
      "  [Batch 440/697] Loss: 4.0469\n",
      "  [Batch 450/697] Loss: 4.3180\n",
      "  [Batch 460/697] Loss: 4.0344\n",
      "  [Batch 470/697] Loss: 4.1553\n",
      "  [Batch 480/697] Loss: 4.3865\n",
      "  [Batch 490/697] Loss: 4.0599\n",
      "  [Batch 500/697] Loss: 4.0579\n",
      "  [Batch 510/697] Loss: 4.2710\n",
      "  [Batch 520/697] Loss: 4.0715\n",
      "  [Batch 530/697] Loss: 4.0592\n",
      "  [Batch 540/697] Loss: 3.8545\n",
      "  [Batch 550/697] Loss: 3.9388\n",
      "  [Batch 560/697] Loss: 4.3080\n",
      "  [Batch 570/697] Loss: 4.0537\n",
      "  [Batch 580/697] Loss: 4.1089\n",
      "  [Batch 590/697] Loss: 4.1068\n",
      "  [Batch 600/697] Loss: 4.1082\n",
      "  [Batch 610/697] Loss: 4.2007\n",
      "  [Batch 620/697] Loss: 3.8452\n",
      "  [Batch 630/697] Loss: 3.8230\n",
      "  [Batch 640/697] Loss: 3.8999\n",
      "  [Batch 650/697] Loss: 3.8307\n",
      "  [Batch 660/697] Loss: 3.8684\n",
      "  [Batch 670/697] Loss: 3.9429\n",
      "  [Batch 680/697] Loss: 3.9279\n",
      "  [Batch 690/697] Loss: 4.0800\n",
      "  [Batch 697/697] Loss: 3.8811\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_1.pth\n",
      "==> Epoch [1/20] Train Loss: 4.4993 | Val Loss: 3.8846 | LR: 0.000994\n",
      "\n",
      "Epoch 2/20\n",
      "  [Batch 10/697] Loss: 3.8260\n",
      "  [Batch 20/697] Loss: 4.0110\n",
      "  [Batch 30/697] Loss: 3.6506\n",
      "  [Batch 40/697] Loss: 3.5075\n",
      "  [Batch 50/697] Loss: 3.7307\n",
      "  [Batch 60/697] Loss: 3.8690\n",
      "  [Batch 70/697] Loss: 3.6666\n",
      "  [Batch 80/697] Loss: 3.9087\n",
      "  [Batch 90/697] Loss: 3.7719\n",
      "  [Batch 100/697] Loss: 3.6325\n",
      "  [Batch 110/697] Loss: 3.6795\n",
      "  [Batch 120/697] Loss: 3.7463\n",
      "  [Batch 130/697] Loss: 3.6333\n",
      "  [Batch 140/697] Loss: 3.4708\n",
      "  [Batch 150/697] Loss: 3.6378\n",
      "  [Batch 160/697] Loss: 3.6043\n",
      "  [Batch 170/697] Loss: 3.2057\n",
      "  [Batch 180/697] Loss: 3.5609\n",
      "  [Batch 190/697] Loss: 3.6202\n",
      "  [Batch 200/697] Loss: 3.5400\n",
      "  [Batch 210/697] Loss: 3.7459\n",
      "  [Batch 220/697] Loss: 3.4643\n",
      "  [Batch 230/697] Loss: 3.5213\n",
      "  [Batch 240/697] Loss: 3.5377\n",
      "  [Batch 250/697] Loss: 3.3971\n",
      "  [Batch 260/697] Loss: 3.4170\n",
      "  [Batch 270/697] Loss: 3.3494\n",
      "  [Batch 280/697] Loss: 3.3063\n",
      "  [Batch 290/697] Loss: 3.2831\n",
      "  [Batch 300/697] Loss: 3.2262\n",
      "  [Batch 310/697] Loss: 3.2346\n",
      "  [Batch 320/697] Loss: 3.3000\n",
      "  [Batch 330/697] Loss: 3.1924\n",
      "  [Batch 340/697] Loss: 3.2282\n",
      "  [Batch 350/697] Loss: 3.2224\n",
      "  [Batch 360/697] Loss: 3.0004\n",
      "  [Batch 370/697] Loss: 3.3665\n",
      "  [Batch 380/697] Loss: 3.3030\n",
      "  [Batch 390/697] Loss: 3.0037\n",
      "  [Batch 400/697] Loss: 3.3908\n",
      "  [Batch 410/697] Loss: 3.1140\n",
      "  [Batch 420/697] Loss: 3.1541\n",
      "  [Batch 430/697] Loss: 3.2705\n",
      "  [Batch 440/697] Loss: 3.0411\n",
      "  [Batch 450/697] Loss: 3.3763\n",
      "  [Batch 460/697] Loss: 3.1804\n",
      "  [Batch 470/697] Loss: 3.0749\n",
      "  [Batch 480/697] Loss: 3.2469\n",
      "  [Batch 490/697] Loss: 3.0825\n",
      "  [Batch 500/697] Loss: 3.0153\n",
      "  [Batch 510/697] Loss: 3.1113\n",
      "  [Batch 520/697] Loss: 2.9106\n",
      "  [Batch 530/697] Loss: 2.9586\n",
      "  [Batch 540/697] Loss: 2.5666\n",
      "  [Batch 550/697] Loss: 3.0879\n",
      "  [Batch 560/697] Loss: 3.1321\n",
      "  [Batch 570/697] Loss: 2.9862\n",
      "  [Batch 580/697] Loss: 2.8876\n",
      "  [Batch 590/697] Loss: 2.7911\n",
      "  [Batch 600/697] Loss: 2.9225\n",
      "  [Batch 610/697] Loss: 3.1215\n",
      "  [Batch 620/697] Loss: 2.6050\n",
      "  [Batch 630/697] Loss: 2.6142\n",
      "  [Batch 640/697] Loss: 2.9227\n",
      "  [Batch 650/697] Loss: 2.6735\n",
      "  [Batch 660/697] Loss: 2.8332\n",
      "  [Batch 670/697] Loss: 2.8090\n",
      "  [Batch 680/697] Loss: 2.6852\n",
      "  [Batch 690/697] Loss: 2.5869\n",
      "  [Batch 697/697] Loss: 2.7600\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_2.pth\n",
      "==> Epoch [2/20] Train Loss: 3.2671 | Val Loss: 2.7615 | LR: 0.000976\n",
      "\n",
      "Epoch 3/20\n",
      "  [Batch 10/697] Loss: 2.4108\n",
      "  [Batch 20/697] Loss: 2.8538\n",
      "  [Batch 30/697] Loss: 2.5843\n",
      "  [Batch 40/697] Loss: 2.2403\n",
      "  [Batch 50/697] Loss: 2.3333\n",
      "  [Batch 60/697] Loss: 2.4597\n",
      "  [Batch 70/697] Loss: 2.5099\n",
      "  [Batch 80/697] Loss: 2.6034\n",
      "  [Batch 90/697] Loss: 2.5512\n",
      "  [Batch 100/697] Loss: 2.3490\n",
      "  [Batch 110/697] Loss: 2.3199\n",
      "  [Batch 120/697] Loss: 2.5119\n",
      "  [Batch 130/697] Loss: 2.2544\n",
      "  [Batch 140/697] Loss: 2.4190\n",
      "  [Batch 150/697] Loss: 2.4365\n",
      "  [Batch 160/697] Loss: 2.3941\n",
      "  [Batch 170/697] Loss: 2.2881\n",
      "  [Batch 180/697] Loss: 2.5181\n",
      "  [Batch 190/697] Loss: 2.1665\n",
      "  [Batch 200/697] Loss: 2.3288\n",
      "  [Batch 210/697] Loss: 2.1426\n",
      "  [Batch 220/697] Loss: 2.2913\n",
      "  [Batch 230/697] Loss: 2.4615\n",
      "  [Batch 240/697] Loss: 2.3842\n",
      "  [Batch 250/697] Loss: 2.4324\n",
      "  [Batch 260/697] Loss: 2.3225\n",
      "  [Batch 270/697] Loss: 2.3505\n",
      "  [Batch 280/697] Loss: 2.3670\n",
      "  [Batch 290/697] Loss: 2.2328\n",
      "  [Batch 300/697] Loss: 2.2357\n",
      "  [Batch 310/697] Loss: 2.2930\n",
      "  [Batch 320/697] Loss: 2.2198\n",
      "  [Batch 330/697] Loss: 2.1577\n",
      "  [Batch 340/697] Loss: 2.1803\n",
      "  [Batch 350/697] Loss: 2.2720\n",
      "  [Batch 360/697] Loss: 2.1761\n",
      "  [Batch 370/697] Loss: 2.1407\n",
      "  [Batch 380/697] Loss: 1.9234\n",
      "  [Batch 390/697] Loss: 1.9925\n",
      "  [Batch 400/697] Loss: 2.1274\n",
      "  [Batch 410/697] Loss: 2.1886\n",
      "  [Batch 420/697] Loss: 1.9726\n",
      "  [Batch 430/697] Loss: 1.9715\n",
      "  [Batch 440/697] Loss: 1.8558\n",
      "  [Batch 450/697] Loss: 2.0143\n",
      "  [Batch 460/697] Loss: 1.9990\n",
      "  [Batch 470/697] Loss: 2.0080\n",
      "  [Batch 480/697] Loss: 1.9361\n",
      "  [Batch 490/697] Loss: 1.8846\n",
      "  [Batch 500/697] Loss: 1.7460\n",
      "  [Batch 510/697] Loss: 2.0131\n",
      "  [Batch 520/697] Loss: 1.8555\n",
      "  [Batch 530/697] Loss: 2.0169\n",
      "  [Batch 540/697] Loss: 1.8975\n",
      "  [Batch 550/697] Loss: 1.9362\n",
      "  [Batch 560/697] Loss: 1.7011\n",
      "  [Batch 570/697] Loss: 1.6519\n",
      "  [Batch 580/697] Loss: 1.9718\n",
      "  [Batch 590/697] Loss: 1.9154\n",
      "  [Batch 600/697] Loss: 1.7424\n",
      "  [Batch 610/697] Loss: 1.9989\n",
      "  [Batch 620/697] Loss: 1.8751\n",
      "  [Batch 630/697] Loss: 1.8409\n",
      "  [Batch 640/697] Loss: 1.7756\n",
      "  [Batch 650/697] Loss: 1.5984\n",
      "  [Batch 660/697] Loss: 1.6566\n",
      "  [Batch 670/697] Loss: 1.5107\n",
      "  [Batch 680/697] Loss: 1.7771\n",
      "  [Batch 690/697] Loss: 1.5863\n",
      "  [Batch 697/697] Loss: 1.5814\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_3.pth\n",
      "==> Epoch [3/20] Train Loss: 2.1365 | Val Loss: 1.6296 | LR: 0.000946\n",
      "\n",
      "Epoch 4/20\n",
      "  [Batch 10/697] Loss: 1.3091\n",
      "  [Batch 20/697] Loss: 1.3208\n",
      "  [Batch 30/697] Loss: 1.7342\n",
      "  [Batch 40/697] Loss: 1.6500\n",
      "  [Batch 50/697] Loss: 1.4089\n",
      "  [Batch 60/697] Loss: 1.4048\n",
      "  [Batch 70/697] Loss: 1.4163\n",
      "  [Batch 80/697] Loss: 1.5770\n",
      "  [Batch 90/697] Loss: 1.3412\n",
      "  [Batch 100/697] Loss: 1.5376\n",
      "  [Batch 110/697] Loss: 1.1927\n",
      "  [Batch 120/697] Loss: 1.5257\n",
      "  [Batch 130/697] Loss: 1.3125\n",
      "  [Batch 140/697] Loss: 1.1170\n",
      "  [Batch 150/697] Loss: 1.3480\n",
      "  [Batch 160/697] Loss: 1.2920\n",
      "  [Batch 170/697] Loss: 1.5175\n",
      "  [Batch 180/697] Loss: 1.4974\n",
      "  [Batch 190/697] Loss: 1.3962\n",
      "  [Batch 200/697] Loss: 1.0780\n",
      "  [Batch 210/697] Loss: 1.1170\n",
      "  [Batch 220/697] Loss: 1.4521\n",
      "  [Batch 230/697] Loss: 1.3455\n",
      "  [Batch 240/697] Loss: 1.3810\n",
      "  [Batch 250/697] Loss: 1.1918\n",
      "  [Batch 260/697] Loss: 1.2293\n",
      "  [Batch 270/697] Loss: 1.3443\n",
      "  [Batch 280/697] Loss: 1.2617\n",
      "  [Batch 290/697] Loss: 1.4176\n",
      "  [Batch 300/697] Loss: 1.4162\n",
      "  [Batch 310/697] Loss: 1.4608\n",
      "  [Batch 320/697] Loss: 1.1758\n",
      "  [Batch 330/697] Loss: 1.2879\n",
      "  [Batch 340/697] Loss: 1.1890\n",
      "  [Batch 350/697] Loss: 1.1386\n",
      "  [Batch 360/697] Loss: 1.2287\n",
      "  [Batch 370/697] Loss: 1.3253\n",
      "  [Batch 380/697] Loss: 1.0453\n",
      "  [Batch 390/697] Loss: 1.0753\n",
      "  [Batch 400/697] Loss: 1.2735\n",
      "  [Batch 410/697] Loss: 1.0829\n",
      "  [Batch 420/697] Loss: 1.1391\n",
      "  [Batch 430/697] Loss: 1.2257\n",
      "  [Batch 440/697] Loss: 1.0734\n",
      "  [Batch 450/697] Loss: 1.1808\n",
      "  [Batch 460/697] Loss: 1.2978\n",
      "  [Batch 470/697] Loss: 1.0547\n",
      "  [Batch 480/697] Loss: 1.1960\n",
      "  [Batch 490/697] Loss: 1.0529\n",
      "  [Batch 500/697] Loss: 1.2676\n",
      "  [Batch 510/697] Loss: 1.0919\n",
      "  [Batch 520/697] Loss: 1.0393\n",
      "  [Batch 530/697] Loss: 1.1024\n",
      "  [Batch 540/697] Loss: 1.1755\n",
      "  [Batch 550/697] Loss: 1.0011\n",
      "  [Batch 560/697] Loss: 1.2029\n",
      "  [Batch 570/697] Loss: 1.0204\n",
      "  [Batch 580/697] Loss: 0.9902\n",
      "  [Batch 590/697] Loss: 1.1270\n",
      "  [Batch 600/697] Loss: 1.0871\n",
      "  [Batch 610/697] Loss: 0.8610\n",
      "  [Batch 620/697] Loss: 1.1455\n",
      "  [Batch 630/697] Loss: 1.2398\n",
      "  [Batch 640/697] Loss: 1.0931\n",
      "  [Batch 650/697] Loss: 1.0463\n",
      "  [Batch 660/697] Loss: 0.9661\n",
      "  [Batch 670/697] Loss: 0.8666\n",
      "  [Batch 680/697] Loss: 0.7068\n",
      "  [Batch 690/697] Loss: 0.9450\n",
      "  [Batch 697/697] Loss: 0.7635\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_4.pth\n",
      "==> Epoch [4/20] Train Loss: 1.2380 | Val Loss: 0.8743 | LR: 0.000905\n",
      "\n",
      "Epoch 5/20\n",
      "  [Batch 10/697] Loss: 1.0177\n",
      "  [Batch 20/697] Loss: 0.6314\n",
      "  [Batch 30/697] Loss: 0.8094\n",
      "  [Batch 40/697] Loss: 0.7843\n",
      "  [Batch 50/697] Loss: 0.7671\n",
      "  [Batch 60/697] Loss: 0.8918\n",
      "  [Batch 70/697] Loss: 0.9809\n",
      "  [Batch 80/697] Loss: 0.8152\n",
      "  [Batch 90/697] Loss: 0.8952\n",
      "  [Batch 100/697] Loss: 0.7811\n",
      "  [Batch 110/697] Loss: 0.8277\n",
      "  [Batch 120/697] Loss: 0.9899\n",
      "  [Batch 130/697] Loss: 0.6857\n",
      "  [Batch 140/697] Loss: 0.8082\n",
      "  [Batch 150/697] Loss: 0.7117\n",
      "  [Batch 160/697] Loss: 0.9107\n",
      "  [Batch 170/697] Loss: 0.7262\n",
      "  [Batch 180/697] Loss: 0.7527\n",
      "  [Batch 190/697] Loss: 0.7509\n",
      "  [Batch 200/697] Loss: 0.7528\n",
      "  [Batch 210/697] Loss: 0.7389\n",
      "  [Batch 220/697] Loss: 0.6876\n",
      "  [Batch 230/697] Loss: 0.5763\n",
      "  [Batch 240/697] Loss: 0.9342\n",
      "  [Batch 250/697] Loss: 0.6913\n",
      "  [Batch 260/697] Loss: 0.8059\n",
      "  [Batch 270/697] Loss: 0.7374\n",
      "  [Batch 280/697] Loss: 0.9463\n",
      "  [Batch 290/697] Loss: 0.7565\n",
      "  [Batch 300/697] Loss: 0.7599\n",
      "  [Batch 310/697] Loss: 0.7185\n",
      "  [Batch 320/697] Loss: 0.4977\n",
      "  [Batch 330/697] Loss: 0.6845\n",
      "  [Batch 340/697] Loss: 0.7187\n",
      "  [Batch 350/697] Loss: 0.6704\n",
      "  [Batch 360/697] Loss: 0.6175\n",
      "  [Batch 370/697] Loss: 0.5858\n",
      "  [Batch 380/697] Loss: 0.7039\n",
      "  [Batch 390/697] Loss: 0.5759\n",
      "  [Batch 400/697] Loss: 0.5765\n",
      "  [Batch 410/697] Loss: 0.6962\n",
      "  [Batch 420/697] Loss: 0.6168\n",
      "  [Batch 430/697] Loss: 0.5579\n",
      "  [Batch 440/697] Loss: 0.7153\n",
      "  [Batch 450/697] Loss: 0.5894\n",
      "  [Batch 460/697] Loss: 0.4952\n",
      "  [Batch 470/697] Loss: 0.6563\n",
      "  [Batch 480/697] Loss: 0.4432\n",
      "  [Batch 490/697] Loss: 0.4883\n",
      "  [Batch 500/697] Loss: 0.6584\n",
      "  [Batch 510/697] Loss: 0.6505\n",
      "  [Batch 520/697] Loss: 0.7174\n",
      "  [Batch 530/697] Loss: 0.6208\n",
      "  [Batch 540/697] Loss: 0.5310\n",
      "  [Batch 550/697] Loss: 0.5387\n",
      "  [Batch 560/697] Loss: 0.6315\n",
      "  [Batch 570/697] Loss: 0.5634\n",
      "  [Batch 580/697] Loss: 0.5969\n",
      "  [Batch 590/697] Loss: 0.7419\n",
      "  [Batch 600/697] Loss: 0.6248\n",
      "  [Batch 610/697] Loss: 0.4635\n",
      "  [Batch 620/697] Loss: 0.6997\n",
      "  [Batch 630/697] Loss: 0.5939\n",
      "  [Batch 640/697] Loss: 0.4716\n",
      "  [Batch 650/697] Loss: 0.5380\n",
      "  [Batch 660/697] Loss: 0.6430\n",
      "  [Batch 670/697] Loss: 0.4672\n",
      "  [Batch 680/697] Loss: 0.6869\n",
      "  [Batch 690/697] Loss: 0.4944\n",
      "  [Batch 697/697] Loss: 0.6700\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_5.pth\n",
      "==> Epoch [5/20] Train Loss: 0.6687 | Val Loss: 0.5602 | LR: 0.000854\n",
      "\n",
      "Epoch 6/20\n",
      "  [Batch 10/697] Loss: 0.4395\n",
      "  [Batch 20/697] Loss: 0.4505\n",
      "  [Batch 30/697] Loss: 0.5071\n",
      "  [Batch 40/697] Loss: 0.3276\n",
      "  [Batch 50/697] Loss: 0.3360\n",
      "  [Batch 60/697] Loss: 0.3125\n",
      "  [Batch 70/697] Loss: 0.4244\n",
      "  [Batch 80/697] Loss: 0.3853\n",
      "  [Batch 90/697] Loss: 0.3306\n",
      "  [Batch 100/697] Loss: 0.4342\n",
      "  [Batch 110/697] Loss: 0.4438\n",
      "  [Batch 120/697] Loss: 0.4263\n",
      "  [Batch 130/697] Loss: 0.3460\n",
      "  [Batch 140/697] Loss: 0.5203\n",
      "  [Batch 150/697] Loss: 0.3147\n",
      "  [Batch 160/697] Loss: 0.3762\n",
      "  [Batch 170/697] Loss: 0.3878\n",
      "  [Batch 180/697] Loss: 0.3460\n",
      "  [Batch 190/697] Loss: 0.4413\n",
      "  [Batch 200/697] Loss: 0.3923\n",
      "  [Batch 210/697] Loss: 0.3446\n",
      "  [Batch 220/697] Loss: 0.3807\n",
      "  [Batch 230/697] Loss: 0.3684\n",
      "  [Batch 240/697] Loss: 0.3957\n",
      "  [Batch 250/697] Loss: 0.3560\n",
      "  [Batch 260/697] Loss: 0.5176\n",
      "  [Batch 270/697] Loss: 0.3076\n",
      "  [Batch 280/697] Loss: 0.3463\n",
      "  [Batch 290/697] Loss: 0.2789\n",
      "  [Batch 300/697] Loss: 0.3122\n",
      "  [Batch 310/697] Loss: 0.3253\n",
      "  [Batch 320/697] Loss: 0.2973\n",
      "  [Batch 330/697] Loss: 0.3928\n",
      "  [Batch 340/697] Loss: 0.3640\n",
      "  [Batch 350/697] Loss: 0.2756\n",
      "  [Batch 360/697] Loss: 0.4209\n",
      "  [Batch 370/697] Loss: 0.4300\n",
      "  [Batch 380/697] Loss: 0.3498\n",
      "  [Batch 390/697] Loss: 0.3920\n",
      "  [Batch 400/697] Loss: 0.3226\n",
      "  [Batch 410/697] Loss: 0.4480\n",
      "  [Batch 420/697] Loss: 0.2777\n",
      "  [Batch 430/697] Loss: 0.4167\n",
      "  [Batch 440/697] Loss: 0.3542\n",
      "  [Batch 450/697] Loss: 0.3482\n",
      "  [Batch 460/697] Loss: 0.3293\n",
      "  [Batch 470/697] Loss: 0.3322\n",
      "  [Batch 480/697] Loss: 0.3374\n",
      "  [Batch 490/697] Loss: 0.4824\n",
      "  [Batch 500/697] Loss: 0.4981\n",
      "  [Batch 510/697] Loss: 0.3873\n",
      "  [Batch 520/697] Loss: 0.4108\n",
      "  [Batch 530/697] Loss: 0.4394\n",
      "  [Batch 540/697] Loss: 0.4047\n",
      "  [Batch 550/697] Loss: 0.2403\n",
      "  [Batch 560/697] Loss: 0.2394\n",
      "  [Batch 570/697] Loss: 0.4293\n",
      "  [Batch 580/697] Loss: 0.2237\n",
      "  [Batch 590/697] Loss: 0.2956\n",
      "  [Batch 600/697] Loss: 0.2967\n",
      "  [Batch 610/697] Loss: 0.3719\n",
      "  [Batch 620/697] Loss: 0.4053\n",
      "  [Batch 630/697] Loss: 0.2283\n",
      "  [Batch 640/697] Loss: 0.3069\n",
      "  [Batch 650/697] Loss: 0.2446\n",
      "  [Batch 660/697] Loss: 0.3686\n",
      "  [Batch 670/697] Loss: 0.2702\n",
      "  [Batch 680/697] Loss: 0.3880\n",
      "  [Batch 690/697] Loss: 0.2945\n",
      "  [Batch 697/697] Loss: 0.4230\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_6.pth\n",
      "==> Epoch [6/20] Train Loss: 0.3747 | Val Loss: 0.3343 | LR: 0.000794\n",
      "\n",
      "Epoch 7/20\n",
      "  [Batch 10/697] Loss: 0.3329\n",
      "  [Batch 20/697] Loss: 0.2667\n",
      "  [Batch 30/697] Loss: 0.1371\n",
      "  [Batch 40/697] Loss: 0.2357\n",
      "  [Batch 50/697] Loss: 0.1665\n",
      "  [Batch 60/697] Loss: 0.1749\n",
      "  [Batch 70/697] Loss: 0.2242\n",
      "  [Batch 80/697] Loss: 0.1342\n",
      "  [Batch 90/697] Loss: 0.2121\n",
      "  [Batch 100/697] Loss: 0.2615\n",
      "  [Batch 110/697] Loss: 0.2467\n",
      "  [Batch 120/697] Loss: 0.1786\n",
      "  [Batch 130/697] Loss: 0.1467\n",
      "  [Batch 140/697] Loss: 0.1489\n",
      "  [Batch 150/697] Loss: 0.1865\n",
      "  [Batch 160/697] Loss: 0.2142\n",
      "  [Batch 170/697] Loss: 0.3013\n",
      "  [Batch 180/697] Loss: 0.1428\n",
      "  [Batch 190/697] Loss: 0.2139\n",
      "  [Batch 200/697] Loss: 0.2648\n",
      "  [Batch 210/697] Loss: 0.2543\n",
      "  [Batch 220/697] Loss: 0.2553\n",
      "  [Batch 230/697] Loss: 0.2558\n",
      "  [Batch 240/697] Loss: 0.3016\n",
      "  [Batch 250/697] Loss: 0.1898\n",
      "  [Batch 260/697] Loss: 0.1942\n",
      "  [Batch 270/697] Loss: 0.2706\n",
      "  [Batch 280/697] Loss: 0.1587\n",
      "  [Batch 290/697] Loss: 0.3001\n",
      "  [Batch 300/697] Loss: 0.1686\n",
      "  [Batch 310/697] Loss: 0.2325\n",
      "  [Batch 320/697] Loss: 0.2659\n",
      "  [Batch 330/697] Loss: 0.1961\n",
      "  [Batch 340/697] Loss: 0.1221\n",
      "  [Batch 350/697] Loss: 0.2238\n",
      "  [Batch 360/697] Loss: 0.1312\n",
      "  [Batch 370/697] Loss: 0.2421\n",
      "  [Batch 380/697] Loss: 0.2883\n",
      "  [Batch 390/697] Loss: 0.2441\n",
      "  [Batch 400/697] Loss: 0.2176\n",
      "  [Batch 410/697] Loss: 0.2206\n",
      "  [Batch 420/697] Loss: 0.1661\n",
      "  [Batch 430/697] Loss: 0.2530\n",
      "  [Batch 440/697] Loss: 0.2122\n",
      "  [Batch 450/697] Loss: 0.2369\n",
      "  [Batch 460/697] Loss: 0.2324\n",
      "  [Batch 470/697] Loss: 0.2089\n",
      "  [Batch 480/697] Loss: 0.2833\n",
      "  [Batch 490/697] Loss: 0.2116\n",
      "  [Batch 500/697] Loss: 0.2206\n",
      "  [Batch 510/697] Loss: 0.2940\n",
      "  [Batch 520/697] Loss: 0.1499\n",
      "  [Batch 530/697] Loss: 0.2453\n",
      "  [Batch 540/697] Loss: 0.1672\n",
      "  [Batch 550/697] Loss: 0.3092\n",
      "  [Batch 560/697] Loss: 0.2529\n",
      "  [Batch 570/697] Loss: 0.1614\n",
      "  [Batch 580/697] Loss: 0.1496\n",
      "  [Batch 590/697] Loss: 0.1775\n",
      "  [Batch 600/697] Loss: 0.1631\n",
      "  [Batch 610/697] Loss: 0.1108\n",
      "  [Batch 620/697] Loss: 0.1518\n",
      "  [Batch 630/697] Loss: 0.2028\n",
      "  [Batch 640/697] Loss: 0.2707\n",
      "  [Batch 650/697] Loss: 0.2623\n",
      "  [Batch 660/697] Loss: 0.1816\n",
      "  [Batch 670/697] Loss: 0.1770\n",
      "  [Batch 680/697] Loss: 0.2076\n",
      "  [Batch 690/697] Loss: 0.1874\n",
      "  [Batch 697/697] Loss: 0.3769\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_7.pth\n",
      "==> Epoch [7/20] Train Loss: 0.2268 | Val Loss: 0.2150 | LR: 0.000727\n",
      "\n",
      "Epoch 8/20\n",
      "  [Batch 10/697] Loss: 0.1496\n",
      "  [Batch 20/697] Loss: 0.0955\n",
      "  [Batch 30/697] Loss: 0.1542\n",
      "  [Batch 40/697] Loss: 0.2371\n",
      "  [Batch 50/697] Loss: 0.1791\n",
      "  [Batch 60/697] Loss: 0.1131\n",
      "  [Batch 70/697] Loss: 0.1722\n",
      "  [Batch 80/697] Loss: 0.1636\n",
      "  [Batch 90/697] Loss: 0.1993\n",
      "  [Batch 100/697] Loss: 0.0952\n",
      "  [Batch 110/697] Loss: 0.1336\n",
      "  [Batch 120/697] Loss: 0.0896\n",
      "  [Batch 130/697] Loss: 0.1160\n",
      "  [Batch 140/697] Loss: 0.2177\n",
      "  [Batch 150/697] Loss: 0.1400\n",
      "  [Batch 160/697] Loss: 0.2248\n",
      "  [Batch 170/697] Loss: 0.1475\n",
      "  [Batch 180/697] Loss: 0.2179\n",
      "  [Batch 190/697] Loss: 0.1670\n",
      "  [Batch 200/697] Loss: 0.2493\n",
      "  [Batch 210/697] Loss: 0.0971\n",
      "  [Batch 220/697] Loss: 0.2161\n",
      "  [Batch 230/697] Loss: 0.1689\n",
      "  [Batch 240/697] Loss: 0.1304\n",
      "  [Batch 250/697] Loss: 0.1795\n",
      "  [Batch 260/697] Loss: 0.1799\n",
      "  [Batch 270/697] Loss: 0.1418\n",
      "  [Batch 280/697] Loss: 0.1067\n",
      "  [Batch 290/697] Loss: 0.1283\n",
      "  [Batch 300/697] Loss: 0.0899\n",
      "  [Batch 310/697] Loss: 0.1663\n",
      "  [Batch 320/697] Loss: 0.1845\n",
      "  [Batch 330/697] Loss: 0.1204\n",
      "  [Batch 340/697] Loss: 0.1891\n",
      "  [Batch 350/697] Loss: 0.1493\n",
      "  [Batch 360/697] Loss: 0.1931\n",
      "  [Batch 370/697] Loss: 0.1790\n",
      "  [Batch 380/697] Loss: 0.1255\n",
      "  [Batch 390/697] Loss: 0.2141\n",
      "  [Batch 400/697] Loss: 0.2138\n",
      "  [Batch 410/697] Loss: 0.1946\n",
      "  [Batch 420/697] Loss: 0.1136\n",
      "  [Batch 430/697] Loss: 0.1464\n",
      "  [Batch 440/697] Loss: 0.2432\n",
      "  [Batch 450/697] Loss: 0.2520\n",
      "  [Batch 460/697] Loss: 0.1729\n",
      "  [Batch 470/697] Loss: 0.0794\n",
      "  [Batch 480/697] Loss: 0.1783\n",
      "  [Batch 490/697] Loss: 0.1516\n",
      "  [Batch 500/697] Loss: 0.1294\n",
      "  [Batch 510/697] Loss: 0.1687\n",
      "  [Batch 520/697] Loss: 0.0956\n",
      "  [Batch 530/697] Loss: 0.1421\n",
      "  [Batch 540/697] Loss: 0.1281\n",
      "  [Batch 550/697] Loss: 0.1865\n",
      "  [Batch 560/697] Loss: 0.1844\n",
      "  [Batch 570/697] Loss: 0.1489\n",
      "  [Batch 580/697] Loss: 0.0942\n",
      "  [Batch 590/697] Loss: 0.2089\n",
      "  [Batch 600/697] Loss: 0.1086\n",
      "  [Batch 610/697] Loss: 0.1848\n",
      "  [Batch 620/697] Loss: 0.1827\n",
      "  [Batch 630/697] Loss: 0.1581\n",
      "  [Batch 640/697] Loss: 0.2681\n",
      "  [Batch 650/697] Loss: 0.1066\n",
      "  [Batch 660/697] Loss: 0.1091\n",
      "  [Batch 670/697] Loss: 0.1273\n",
      "  [Batch 680/697] Loss: 0.1398\n",
      "  [Batch 690/697] Loss: 0.1651\n",
      "  [Batch 697/697] Loss: 0.1387\n",
      "Checkpoint saved: ./checkpoint_efficientnet_b3/checkpoint_epoch_8.pth\n",
      "==> Epoch [8/20] Train Loss: 0.1589 | Val Loss: 0.1290 | LR: 0.000655\n",
      "\n",
      "Epoch 9/20\n",
      "  [Batch 10/697] Loss: 0.1432\n",
      "  [Batch 20/697] Loss: 0.0488\n",
      "  [Batch 30/697] Loss: 0.1020\n",
      "  [Batch 40/697] Loss: 0.1209\n",
      "  [Batch 50/697] Loss: 0.0469\n",
      "  [Batch 60/697] Loss: 0.0360\n",
      "  [Batch 70/697] Loss: 0.0763\n",
      "  [Batch 80/697] Loss: 0.0410\n",
      "  [Batch 90/697] Loss: 0.0723\n",
      "  [Batch 100/697] Loss: 0.1462\n",
      "  [Batch 110/697] Loss: 0.1064\n",
      "  [Batch 120/697] Loss: 0.0463\n",
      "  [Batch 130/697] Loss: 0.1503\n",
      "  [Batch 140/697] Loss: 0.1404\n",
      "  [Batch 150/697] Loss: 0.0664\n",
      "  [Batch 160/697] Loss: 0.1446\n",
      "  [Batch 170/697] Loss: 0.1026\n",
      "  [Batch 180/697] Loss: 0.0495\n",
      "  [Batch 190/697] Loss: 0.0472\n",
      "  [Batch 200/697] Loss: 0.0576\n",
      "  [Batch 210/697] Loss: 0.1242\n",
      "  [Batch 220/697] Loss: 0.1433\n",
      "  [Batch 230/697] Loss: 0.0701\n",
      "  [Batch 240/697] Loss: 0.0887\n",
      "  [Batch 250/697] Loss: 0.1638\n",
      "  [Batch 260/697] Loss: 0.0809\n",
      "  [Batch 270/697] Loss: 0.1445\n",
      "  [Batch 280/697] Loss: 0.1565\n",
      "  [Batch 290/697] Loss: 0.0569\n",
      "  [Batch 300/697] Loss: 0.0385\n",
      "  [Batch 310/697] Loss: 0.0849\n",
      "  [Batch 320/697] Loss: 0.1381\n",
      "  [Batch 330/697] Loss: 0.0735\n",
      "  [Batch 340/697] Loss: 0.1155\n",
      "  [Batch 350/697] Loss: 0.1088\n",
      "  [Batch 360/697] Loss: 0.1962\n",
      "  [Batch 370/697] Loss: 0.0664\n",
      "  [Batch 380/697] Loss: 0.1157\n",
      "  [Batch 390/697] Loss: 0.0893\n",
      "  [Batch 400/697] Loss: 0.1756\n",
      "  [Batch 410/697] Loss: 0.1190\n",
      "  [Batch 420/697] Loss: 0.0677\n",
      "  [Batch 430/697] Loss: 0.1038\n",
      "  [Batch 440/697] Loss: 0.1016\n",
      "  [Batch 450/697] Loss: 0.1203\n",
      "  [Batch 460/697] Loss: 0.0528\n",
      "  [Batch 470/697] Loss: 0.0533\n",
      "  [Batch 480/697] Loss: 0.0667\n",
      "  [Batch 490/697] Loss: 0.0571\n",
      "  [Batch 500/697] Loss: 0.0821\n",
      "  [Batch 510/697] Loss: 0.0641\n",
      "  [Batch 520/697] Loss: 0.0648\n",
      "  [Batch 530/697] Loss: 0.1192\n",
      "  [Batch 540/697] Loss: 0.1167\n",
      "  [Batch 550/697] Loss: 0.1533\n",
      "  [Batch 560/697] Loss: 0.0529\n",
      "  [Batch 570/697] Loss: 0.0864\n",
      "  [Batch 580/697] Loss: 0.1188\n",
      "  [Batch 590/697] Loss: 0.1362\n",
      "  [Batch 600/697] Loss: 0.0981\n",
      "  [Batch 610/697] Loss: 0.1197\n",
      "  [Batch 620/697] Loss: 0.0816\n",
      "  [Batch 630/697] Loss: 0.0692\n",
      "  [Batch 640/697] Loss: 0.1144\n",
      "  [Batch 650/697] Loss: 0.0960\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (7) 학습\n",
    "######################################################\n",
    "train_model(epochs)\n",
    "print(\"----- Training Finished -----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2e71a-238d-4945-a7d4-c1bcb0795a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (8) 평가(Eval) & CSV 제출\n",
    "######################################################\n",
    "# (선택) 원하는 체크포인트 로드 - 주석 해제 시 사용\n",
    "# custom_ckpt = os.path.join(checkpoint_dir, \"checkpoint_epoch_14.pth\")\n",
    "# model.load_state_dict(torch.load(custom_ckpt))\n",
    "# print(f\"Loaded checkpoint: {custom_ckpt}\")\n",
    "\n",
    "print(\"[Step 8] Evaluating current model & Saving CSV...\")\n",
    "all_labels_main, all_preds_main = test_model(model, test_loader)\n",
    "\n",
    "submission_main = pd.read_csv('./sample_submission.csv')\n",
    "submission_main['Label'] = all_preds_main\n",
    "submission_main.to_csv('./submission_epochMain.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_epochMain.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d629c-0a3b-46c7-8433-69e7c907567d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1faa23-3705-4c0e-b6a2-3fb2aa4f2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (9) 학습 이후 분석 (혼동행렬 & F1) + 낮은 F1 클래스 폴더로 복사\n",
    "######################################################\n",
    "cm = confusion_matrix(all_labels_main, all_preds_main)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, cmap='Blues', annot=False, fmt='d')\n",
    "plt.title(\"Confusion Matrix (EfficientNet-B3)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=200)\n",
    "plt.close()\n",
    "print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "class_names = test_dataset.classes\n",
    "report_dict = classification_report(all_labels_main, all_preds_main,\n",
    "                                    target_names=class_names,\n",
    "                                    output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df.to_csv(\"class_f1_report.csv\", index=True)\n",
    "print(\"Classification report (including F1) saved as 'class_f1_report.csv'\")\n",
    "\n",
    "class_report_only = report_df.iloc[:num_classes]\n",
    "class_report_only = class_report_only.sort_values(by=\"f1-score\", ascending=True)\n",
    "lowest_30 = class_report_only.head(30)\n",
    "lowest_30_path = \"lowest_30_classes.csv\"\n",
    "lowest_30.to_csv(lowest_30_path)\n",
    "print(f\"Lowest 30 F1-score classes saved as '{lowest_30_path}'\")\n",
    "\n",
    "print(\"\\n--- Lowest 30 Classes by F1-Score ---\")\n",
    "print(lowest_30[[\"precision\", \"recall\", \"f1-score\"]])\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def save_low_f1_images_to_folder(df, dataset, out_root):\n",
    "    low_class_names = df.index.tolist()\n",
    "    c2i = dataset.class_to_idx\n",
    "    i2c = {v: k for k, v in c2i.items()}\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    for path, idx in dataset.samples:\n",
    "        if i2c[idx] in low_class_names:\n",
    "            dst_dir = os.path.join(out_root, i2c[idx])\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            shutil.copy2(path, os.path.join(dst_dir, os.path.basename(path)))\n",
    "\n",
    "save_low_f1_images_to_folder(lowest_30, full_train_dataset, \"lowest_30_data\")\n",
    "print(\"\\n[Step 9] Copied low-F1 class images to 'lowest_30_data' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b6ed-9231-4a15-b68a-7a1c7126f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (10) F1 낮은 클래스 폴더로부터 파인튜닝\n",
    "######################################################\n",
    "def fine_tune_low_f1_classes_folder(\n",
    "    data_root,\n",
    "    transform_train,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    bs,\n",
    "    ckpt_dir\n",
    "):\n",
    "    ds = datasets.ImageFolder(data_root, transform_train)\n",
    "    dl = DataLoader(ds, batch_size=bs, shuffle=True)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        save_path = os.path.join(ckpt_dir, f\"fine_tune_folder_epoch_{e+1}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"[Fine-Tune Folder] Epoch {e+1}/{epochs} | Loss: {total_loss/len(dl):.4f} | Saved: {save_path}\")\n",
    "\n",
    "print(\"[Step 10] Start fine-tuning on 'lowest_30_data'...\")\n",
    "fine_tune_low_f1_classes_folder(\n",
    "    data_root=\"lowest_30_data\",\n",
    "    transform_train=transform_train,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    epochs=2,                # 예: 2 epoch\n",
    "    bs=64,                   # 배치 사이즈\n",
    "    ckpt_dir=checkpoint_dir\n",
    ")\n",
    "print(\"[Step 10] Fine-tuning finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495002d4-6897-4479-9317-880122decbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (11) 최종 평가 (원하면 CSV 생성)\n",
    "######################################################\n",
    "print(\"[Step 11] Testing after fine-tuning...\")\n",
    "all_labels_ft, all_preds_ft = test_model(model, test_loader)\n",
    "\n",
    "# 파인튜닝 후 CSV 저장\n",
    "submission_ft = pd.read_csv('./sample_submission.csv')\n",
    "submission_ft['Label'] = all_preds_ft\n",
    "submission_ft.to_csv('./submission_epochFineTune.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_epochFineTune.csv'.\")\n",
    "print(\"----- Done -----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1_pytorch_2.5.1",
   "language": "python",
   "name": "1_pytorch_2.5.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
