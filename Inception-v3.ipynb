{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063195f8-7fd3-4bba-98fa-59a4c9ba0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (0) 초기 설정\n",
    "######################################################\n",
    "import os, random, torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    재현성(일관된 결과)을 위해 SEED 고정하는 함수\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 예시: SEED 123\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44191105-cd8d-4716-be6d-eb108d01760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (1) 라이브러리 임포트 및 기본 설정\n",
    "######################################################\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 가능 시 'cuda', 아니면 'cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e03ba8-8e4b-41c6-9186-4bb982dbb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (2) 하이퍼파라미터 & 경로 설정\n",
    "######################################################\n",
    "batch_size = 64 # 메모리 보면서 조절하세요\n",
    "epochs = 50        # 예시로 20 epoch\n",
    "learning_rate = 0.001\n",
    "num_classes = 300\n",
    "weight_decay = 1e-4                                                           # 기존 코드에서 weight_decay 없음. 이건 사용 안할 예정\n",
    "\n",
    "# (사용 환경에 맞게 꼭 수정)\n",
    "train_path = \"/home/student/workspace/data/train\"\n",
    "test_path  = \"/test/final_exam/challenge/test\"\n",
    "\n",
    "# 체크포인트 저장 폴더 (폴더명만 변경)\n",
    "checkpoint_dir = \"./checkpoint_inception_v3_je_condition\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 결과 저장 시 모델 이름 표기를 위한 변수\n",
    "model_name = \"inception_v3\"  # ★ 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb31c47-a451-435e-bcc0-be0b4cad4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   89179\n",
      "Validation samples: 4694\n",
      "Test samples:       4178\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (3) 데이터 변환 & 데이터셋/로더 정의\n",
    "######################################################\n",
    "# ★ Inception v3는 보통 299×299 입력 권장.\n",
    "#   필요시 다른 해상도 쓰셔도 됩니다.\n",
    "transform_train = transforms.Compose([                       # 기존 코드에서 512, 512 등 모든 걸 다 가져왔음\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Train / Test 데이터셋\n",
    "full_train_dataset = datasets.ImageFolder(train_path, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(test_path,  transform=transform_test)\n",
    "\n",
    "# train, valid 층화추출 여기서 부터 수정\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Dataset 전체의 클래스 레이블 가져오기\n",
    "targets = [full_train_dataset.targets[i] for i in range(len(full_train_dataset))]\n",
    "\n",
    "# train, valid 셋 클래스 비율을 맞추기 위한 층화추출\n",
    "# 층화추출을 위한 StratifiedShuffleSplit 설정\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=42)  # train: 80%, valid: 20%\n",
    "\n",
    "# train_index와 valid_index 생성\n",
    "for train_index, valid_index in split.split(np.zeros(len(targets)), targets):\n",
    "    train_indices = train_index\n",
    "    valid_indices = valid_index\n",
    "\n",
    "# Subset을 사용하여 Train과 Valid Dataset 생성\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "valid_dataset = Subset(full_train_dataset, valid_indices)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# train, valid 셋 클래스 비율을 맞추기 위한 층화추출 확인\n",
    "print(pd.Series(full_train_dataset.targets).value_counts(normalize=True, ascending=True))\n",
    "print()\n",
    "print(pd.Series(train_dataset.dataset.targets).value_counts(normalize=True, ascending=True))\n",
    "print()\n",
    "print(pd.Series(valid_dataset.dataset.targets).value_counts(normalize=True, ascending=True))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" 기존 split\n",
    "# Validation split (예: 5%)\n",
    "train_size = int(0.95 * len(full_train_dataset))\n",
    "val_size   = len(full_train_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader      = DataLoader(train_dataset,      batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader       = DataLoader(test_dataset,       batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(validation_dataset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88ef7b0-2ecf-4a77-bba4-ef002e3f023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (4) 모델 정의 (Inception v3, scratch) & (선택)체크포인트 로드\n",
    "######################################################\n",
    "# ★ Inception v3 불러오기 → aux_logits=False 권장\n",
    "\n",
    "model = models.inception_v3(pretrained=False)    # 기존 코드에서 이렇게 정의해서 변경함.\n",
    "# model.fc = nn.Linear(model.fc.\n",
    "model.fc = nn.Linear(model.fc.in_features, 300, bias= True)\n",
    "model = model.to(device)\n",
    "\n",
    "# [원하는 체크포인트 로드 시 주석 해제 예시]\n",
    "# ckpt_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_14.pth\")\n",
    "# model.load_state_dict(torch.load(ckpt_path))\n",
    "# print(f\"Loaded checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03570a70-ffb5-41af-8617-20f42a89ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (5) 옵티마이저, 스케줄러, 손실함수 정의\n",
    "######################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ★ Adam → AdamW로 변경\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)              # 기존 코드에서 weight decay는 사용하지 않음. 제거함.\n",
    "\n",
    "# 스케줄러 (CosineAnnealingLR 그대로)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs) # 스케줄러만 고정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13965c60-9ba9-4c2b-8624-c50afc3bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (6) 학습 함수, 검증 함수, 테스트 함수\n",
    "######################################################\n",
    "def train_model(num_epochs=epochs):\n",
    "    \"\"\"\n",
    "    전체 학습 루프:\n",
    "    1. Train 모드로 돌면서 배치마다 Forward → Loss 계산 → Backward → Optimizer update\n",
    "    2. Validation Loss 측정\n",
    "    3. 체크포인트 저장 + 스케줄러 step\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss  = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 10번째 배치마다(or 마지막 배치) 로그 출력\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(f\"  [Batch {batch_idx+1}/{total_batches}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation Loss 체크\n",
    "        val_loss = validate_model()\n",
    "\n",
    "        # 매 Epoch마다 체크포인트 저장\n",
    "        ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "        # 스케줄러 step\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"==> Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {running_loss/total_batches:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "\n",
    "def validate_model():\n",
    "    \"\"\"\n",
    "    Validation 데이터셋을 통해 현재 모델 상태의 Loss 평가\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(validation_loader)\n",
    "\n",
    "def test_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Test/Validation 데이터셋을 평가하여 정확도(Accuracy) 측정\n",
    "    혼동행렬, F1-score 계산 등을 위해 예측값/정답을 리스트로 저장\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    all_preds  = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total   += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # F1, 혼동행렬 등을 위해 저장\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b32bf50-718f-40d5-8aa7-b4a9a6ec48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  [Batch 10/186] Loss: 5.6705\n",
      "  [Batch 20/186] Loss: 5.4064\n",
      "  [Batch 30/186] Loss: 5.0325\n",
      "  [Batch 40/186] Loss: 4.8271\n",
      "  [Batch 50/186] Loss: 4.6020\n",
      "  [Batch 60/186] Loss: 4.6165\n",
      "  [Batch 70/186] Loss: 4.3791\n",
      "  [Batch 80/186] Loss: 4.1928\n",
      "  [Batch 90/186] Loss: 4.1018\n",
      "  [Batch 100/186] Loss: 4.0824\n",
      "  [Batch 110/186] Loss: 3.8271\n",
      "  [Batch 120/186] Loss: 3.7534\n",
      "  [Batch 130/186] Loss: 3.7277\n",
      "  [Batch 140/186] Loss: 3.5102\n",
      "  [Batch 150/186] Loss: 3.5962\n",
      "  [Batch 160/186] Loss: 3.3707\n",
      "  [Batch 170/186] Loss: 3.3567\n",
      "  [Batch 180/186] Loss: 3.3027\n",
      "  [Batch 186/186] Loss: 3.2150\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_1.pth\n",
      "==> Epoch [1/50] Train Loss: 4.2090 | Val Loss: 3.2899 | LR: 0.000999\n",
      "\n",
      "Epoch 2/50\n",
      "  [Batch 10/186] Loss: 3.0222\n",
      "  [Batch 20/186] Loss: 2.8528\n",
      "  [Batch 30/186] Loss: 2.7838\n",
      "  [Batch 40/186] Loss: 2.5894\n",
      "  [Batch 50/186] Loss: 2.7220\n",
      "  [Batch 60/186] Loss: 2.6948\n",
      "  [Batch 70/186] Loss: 2.6259\n",
      "  [Batch 80/186] Loss: 2.3935\n",
      "  [Batch 90/186] Loss: 2.3704\n",
      "  [Batch 100/186] Loss: 2.3213\n",
      "  [Batch 110/186] Loss: 2.2623\n",
      "  [Batch 120/186] Loss: 2.2770\n",
      "  [Batch 130/186] Loss: 2.0408\n",
      "  [Batch 140/186] Loss: 2.0678\n",
      "  [Batch 150/186] Loss: 2.0085\n",
      "  [Batch 160/186] Loss: 1.9207\n",
      "  [Batch 170/186] Loss: 1.8504\n",
      "  [Batch 180/186] Loss: 1.8943\n",
      "  [Batch 186/186] Loss: 1.9851\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_2.pth\n",
      "==> Epoch [2/50] Train Loss: 2.3891 | Val Loss: 2.4744 | LR: 0.000996\n",
      "\n",
      "Epoch 3/50\n",
      "  [Batch 10/186] Loss: 1.5081\n",
      "  [Batch 20/186] Loss: 1.4983\n",
      "  [Batch 30/186] Loss: 1.5390\n",
      "  [Batch 40/186] Loss: 1.4536\n",
      "  [Batch 50/186] Loss: 1.4732\n",
      "  [Batch 60/186] Loss: 1.4391\n",
      "  [Batch 70/186] Loss: 1.5095\n",
      "  [Batch 80/186] Loss: 1.3676\n",
      "  [Batch 90/186] Loss: 1.5258\n",
      "  [Batch 100/186] Loss: 1.2530\n",
      "  [Batch 110/186] Loss: 1.2623\n",
      "  [Batch 120/186] Loss: 1.1965\n",
      "  [Batch 130/186] Loss: 1.0550\n",
      "  [Batch 140/186] Loss: 0.9816\n",
      "  [Batch 150/186] Loss: 1.0075\n",
      "  [Batch 160/186] Loss: 1.0550\n",
      "  [Batch 170/186] Loss: 0.9822\n",
      "  [Batch 180/186] Loss: 0.8661\n",
      "  [Batch 186/186] Loss: 0.8093\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_3.pth\n",
      "==> Epoch [3/50] Train Loss: 1.2586 | Val Loss: 1.4813 | LR: 0.000991\n",
      "\n",
      "Epoch 4/50\n",
      "  [Batch 10/186] Loss: 0.7651\n",
      "  [Batch 20/186] Loss: 0.7866\n",
      "  [Batch 30/186] Loss: 0.7221\n",
      "  [Batch 40/186] Loss: 0.6821\n",
      "  [Batch 50/186] Loss: 0.7204\n",
      "  [Batch 60/186] Loss: 0.6706\n",
      "  [Batch 70/186] Loss: 0.7127\n",
      "  [Batch 80/186] Loss: 0.5641\n",
      "  [Batch 90/186] Loss: 0.5890\n",
      "  [Batch 100/186] Loss: 0.5985\n",
      "  [Batch 110/186] Loss: 0.5620\n",
      "  [Batch 120/186] Loss: 0.4601\n",
      "  [Batch 130/186] Loss: 0.5416\n",
      "  [Batch 140/186] Loss: 0.5945\n",
      "  [Batch 150/186] Loss: 0.5623\n",
      "  [Batch 160/186] Loss: 0.5239\n",
      "  [Batch 170/186] Loss: 0.4730\n",
      "  [Batch 180/186] Loss: 0.5172\n",
      "  [Batch 186/186] Loss: 0.4553\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_4.pth\n",
      "==> Epoch [4/50] Train Loss: 0.6350 | Val Loss: 0.8826 | LR: 0.000984\n",
      "\n",
      "Epoch 5/50\n",
      "  [Batch 10/186] Loss: 0.3838\n",
      "  [Batch 20/186] Loss: 0.4722\n",
      "  [Batch 30/186] Loss: 0.4570\n",
      "  [Batch 40/186] Loss: 0.3286\n",
      "  [Batch 50/186] Loss: 0.3708\n",
      "  [Batch 60/186] Loss: 0.3367\n",
      "  [Batch 70/186] Loss: 0.3212\n",
      "  [Batch 80/186] Loss: 0.3402\n",
      "  [Batch 90/186] Loss: 0.3057\n",
      "  [Batch 100/186] Loss: 0.3562\n",
      "  [Batch 110/186] Loss: 0.3754\n",
      "  [Batch 120/186] Loss: 0.3386\n",
      "  [Batch 130/186] Loss: 0.3476\n",
      "  [Batch 140/186] Loss: 0.3760\n",
      "  [Batch 150/186] Loss: 0.2583\n",
      "  [Batch 160/186] Loss: 0.3267\n",
      "  [Batch 170/186] Loss: 0.3641\n",
      "  [Batch 180/186] Loss: 0.2879\n",
      "  [Batch 186/186] Loss: 0.3158\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_5.pth\n",
      "==> Epoch [5/50] Train Loss: 0.3553 | Val Loss: 0.5328 | LR: 0.000976\n",
      "\n",
      "Epoch 6/50\n",
      "  [Batch 10/186] Loss: 0.2262\n",
      "  [Batch 20/186] Loss: 0.2257\n",
      "  [Batch 30/186] Loss: 0.2256\n",
      "  [Batch 40/186] Loss: 0.2197\n",
      "  [Batch 50/186] Loss: 0.1964\n",
      "  [Batch 60/186] Loss: 0.1986\n",
      "  [Batch 70/186] Loss: 0.2488\n",
      "  [Batch 80/186] Loss: 0.2237\n",
      "  [Batch 90/186] Loss: 0.2616\n",
      "  [Batch 100/186] Loss: 0.1816\n",
      "  [Batch 110/186] Loss: 0.2245\n",
      "  [Batch 120/186] Loss: 0.2000\n",
      "  [Batch 130/186] Loss: 0.2169\n",
      "  [Batch 140/186] Loss: 0.1543\n",
      "  [Batch 150/186] Loss: 0.2364\n",
      "  [Batch 160/186] Loss: 0.1831\n",
      "  [Batch 170/186] Loss: 0.1801\n",
      "  [Batch 180/186] Loss: 0.2152\n",
      "  [Batch 186/186] Loss: 0.2745\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_6.pth\n",
      "==> Epoch [6/50] Train Loss: 0.2210 | Val Loss: 0.5238 | LR: 0.000965\n",
      "\n",
      "Epoch 7/50\n",
      "  [Batch 10/186] Loss: 0.1946\n",
      "  [Batch 20/186] Loss: 0.1580\n",
      "  [Batch 30/186] Loss: 0.1276\n",
      "  [Batch 40/186] Loss: 0.2041\n",
      "  [Batch 50/186] Loss: 0.1432\n",
      "  [Batch 60/186] Loss: 0.1759\n",
      "  [Batch 70/186] Loss: 0.1601\n",
      "  [Batch 80/186] Loss: 0.1557\n",
      "  [Batch 90/186] Loss: 0.1124\n",
      "  [Batch 100/186] Loss: 0.1193\n",
      "  [Batch 110/186] Loss: 0.1641\n",
      "  [Batch 120/186] Loss: 0.1142\n",
      "  [Batch 130/186] Loss: 0.2082\n",
      "  [Batch 140/186] Loss: 0.1373\n",
      "  [Batch 150/186] Loss: 0.1433\n",
      "  [Batch 160/186] Loss: 0.1046\n",
      "  [Batch 170/186] Loss: 0.1524\n",
      "  [Batch 180/186] Loss: 0.1560\n",
      "  [Batch 186/186] Loss: 0.2122\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_7.pth\n",
      "==> Epoch [7/50] Train Loss: 0.1512 | Val Loss: 0.5438 | LR: 0.000952\n",
      "\n",
      "Epoch 8/50\n",
      "  [Batch 10/186] Loss: 0.1512\n",
      "  [Batch 20/186] Loss: 0.1281\n",
      "  [Batch 30/186] Loss: 0.1317\n",
      "  [Batch 40/186] Loss: 0.1407\n",
      "  [Batch 50/186] Loss: 0.1216\n",
      "  [Batch 60/186] Loss: 0.1160\n",
      "  [Batch 70/186] Loss: 0.0895\n",
      "  [Batch 80/186] Loss: 0.1504\n",
      "  [Batch 90/186] Loss: 0.0966\n",
      "  [Batch 100/186] Loss: 0.1377\n",
      "  [Batch 110/186] Loss: 0.0914\n",
      "  [Batch 120/186] Loss: 0.0921\n",
      "  [Batch 130/186] Loss: 0.0732\n",
      "  [Batch 140/186] Loss: 0.0971\n",
      "  [Batch 150/186] Loss: 0.1132\n",
      "  [Batch 160/186] Loss: 0.1063\n",
      "  [Batch 170/186] Loss: 0.1077\n",
      "  [Batch 180/186] Loss: 0.1175\n",
      "  [Batch 186/186] Loss: 0.1448\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_8.pth\n",
      "==> Epoch [8/50] Train Loss: 0.1178 | Val Loss: 0.2779 | LR: 0.000938\n",
      "\n",
      "Epoch 9/50\n",
      "  [Batch 10/186] Loss: 0.1008\n",
      "  [Batch 20/186] Loss: 0.1134\n",
      "  [Batch 30/186] Loss: 0.1120\n",
      "  [Batch 40/186] Loss: 0.1119\n",
      "  [Batch 50/186] Loss: 0.1080\n",
      "  [Batch 60/186] Loss: 0.0751\n",
      "  [Batch 70/186] Loss: 0.1809\n",
      "  [Batch 80/186] Loss: 0.0935\n",
      "  [Batch 90/186] Loss: 0.1074\n",
      "  [Batch 100/186] Loss: 0.1138\n",
      "  [Batch 110/186] Loss: 0.0941\n",
      "  [Batch 120/186] Loss: 0.0683\n",
      "  [Batch 130/186] Loss: 0.0814\n",
      "  [Batch 140/186] Loss: 0.0549\n",
      "  [Batch 150/186] Loss: 0.0913\n",
      "  [Batch 160/186] Loss: 0.0794\n",
      "  [Batch 170/186] Loss: 0.1135\n",
      "  [Batch 180/186] Loss: 0.0890\n",
      "  [Batch 186/186] Loss: 0.1105\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_9.pth\n",
      "==> Epoch [9/50] Train Loss: 0.1038 | Val Loss: 0.3514 | LR: 0.000922\n",
      "\n",
      "Epoch 10/50\n",
      "  [Batch 10/186] Loss: 0.0859\n",
      "  [Batch 20/186] Loss: 0.0963\n",
      "  [Batch 30/186] Loss: 0.0839\n",
      "  [Batch 40/186] Loss: 0.0847\n",
      "  [Batch 50/186] Loss: 0.0768\n",
      "  [Batch 60/186] Loss: 0.0703\n",
      "  [Batch 70/186] Loss: 0.0603\n",
      "  [Batch 80/186] Loss: 0.0819\n",
      "  [Batch 90/186] Loss: 0.0872\n",
      "  [Batch 100/186] Loss: 0.0960\n",
      "  [Batch 110/186] Loss: 0.0914\n",
      "  [Batch 120/186] Loss: 0.0522\n",
      "  [Batch 130/186] Loss: 0.0623\n",
      "  [Batch 140/186] Loss: 0.0463\n",
      "  [Batch 150/186] Loss: 0.0809\n",
      "  [Batch 160/186] Loss: 0.0605\n",
      "  [Batch 170/186] Loss: 0.0855\n",
      "  [Batch 180/186] Loss: 0.0686\n",
      "  [Batch 186/186] Loss: 0.0922\n",
      "Checkpoint saved: ./checkpoint_inception_v3/checkpoint_epoch_10.pth\n",
      "==> Epoch [10/50] Train Loss: 0.0753 | Val Loss: 0.2229 | LR: 0.000905\n",
      "\n",
      "Epoch 11/50\n",
      "  [Batch 10/186] Loss: 0.1065\n",
      "  [Batch 20/186] Loss: 0.0485\n",
      "  [Batch 30/186] Loss: 0.0653\n",
      "  [Batch 40/186] Loss: 0.0737\n",
      "  [Batch 50/186] Loss: 0.0530\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m######################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (7) 학습\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m######################################################\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----- Training Finished -----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/1_pytorch_2.5.1/lib/python3.10/site-packages/torchvision/transforms/functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# (7) 학습\n",
    "######################################################\n",
    "train_model(epochs)\n",
    "print(\"----- Training Finished -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2e71a-238d-4945-a7d4-c1bcb0795a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (8) 평가(Eval) & CSV 제출\n",
    "######################################################\n",
    "# (선택) 원하는 체크포인트 로드 (주석 해제 시 사용)\n",
    "# custom_ckpt = os.path.join(checkpoint_dir, \"checkpoint_epoch_14.pth\")\n",
    "# model.load_state_dict(torch.load(custom_ckpt))\n",
    "# print(f\"Loaded checkpoint: {custom_ckpt}\")\n",
    "\n",
    "print(\"[Step 8] Evaluating current model & Saving CSV...\")\n",
    "all_labels_main, all_preds_main = test_model(model, test_loader)\n",
    "\n",
    "# 제출용 CSV (sample_submission.csv 기반)\n",
    "submission_main = pd.read_csv('./sample_submission.csv')\n",
    "submission_main['Label'] = all_preds_main\n",
    "submission_main.to_csv('./submission_epochMain.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_epochMain.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d629c-0a3b-46c7-8433-69e7c907567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (9) 혼동행렬 / F1-score 계산 및 저장 -> 혼동 행렬 부분 좀 더 수정할 필요가 있음. 가중치 들고와서 할 수 있도록. 이건 나중에 하면 됩니다.\n",
    "######################################################\n",
    "# 클래스 이름 (훈련 데이터 기준 가정)\n",
    "class_names = [f\"class_{i}\" for i in range(num_classes)]\n",
    "\n",
    "# (1) 혼동 행렬 생성 및 저장\n",
    "cm = confusion_matrix(all_labels_main, all_preds_main)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, cmap='Blues', annot=False, fmt='d', cbar_kws={\"label\": \"Frequency\"})\n",
    "plt.title(f\"Confusion Matrix ({model_name} - {epochs} Epochs)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(f\"confusion_matrix_{model_name}_epochs{epochs}.png\", dpi=200)\n",
    "plt.close()\n",
    "print(f\"Confusion matrix saved as 'confusion_matrix_{model_name}_epochs{epochs}.png'\")\n",
    "\n",
    "# (2) Validation 데이터 기준 F1-score 계산 및 저장\n",
    "val_labels, val_preds = test_model(model, validation_loader)  # Validation 데이터로 예측\n",
    "report_dict = classification_report(val_labels, val_preds,\n",
    "                                    target_names=class_names,\n",
    "                                    output_dict=True)\n",
    "\n",
    "# F1 보고서 DataFrame 생성\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_path = f\"val_f1_report_{model_name}_epochs{epochs}.csv\"\n",
    "report_df.to_csv(report_path, index=True)\n",
    "print(f\"Validation F1-score report saved as '{report_path}'\")\n",
    "\n",
    "# (3) F1-score 낮은 클래스 50개 선택 및 CSV 저장\n",
    "class_report_only = report_df.iloc[:num_classes]  # 300개 클래스만 선택\n",
    "class_report_only = class_report_only.sort_values(by=\"f1-score\", ascending=True)\n",
    "lowest_50 = class_report_only.head(50)\n",
    "lowest_50_path = f\"val_lowest_50_classes_{model_name}_epochs{epochs}.csv\"\n",
    "lowest_50.to_csv(lowest_50_path)\n",
    "print(f\"Lowest 50 F1-score classes (Validation) saved as '{lowest_50_path}'\")\n",
    "\n",
    "print(\"\\n--- Lowest 50 Classes by F1-Score (Validation) ---\")\n",
    "print(lowest_50[[\"precision\", \"recall\", \"f1-score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b6ed-9231-4a15-b68a-7a1c7126f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (10) F1 낮은 클래스 폴더로부터 파인튜닝 -> 여기서 부턴 좀 더 고려해봐야합니다.\n",
    "######################################################\n",
    "def fine_tune_low_f1_classes_folder(\n",
    "    data_root,\n",
    "    transform_train,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    bs,\n",
    "    ckpt_dir\n",
    "):\n",
    "    ds = datasets.ImageFolder(data_root, transform_train)\n",
    "    dl = DataLoader(ds, batch_size=bs, shuffle=True)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        save_path = os.path.join(ckpt_dir, f\"fine_tune_folder_epoch_{e+1}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"[Fine-Tune Folder] Epoch {e+1}/{epochs} | Loss: {total_loss/len(dl):.4f} | Saved: {save_path}\")\n",
    "\n",
    "print(\"[Step 10] Start fine-tuning on 'lowest_30_data'...\")\n",
    "fine_tune_low_f1_classes_folder(\n",
    "    data_root=\"lowest_30_data\",\n",
    "    transform_train=transform_train,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    epochs=2,                # 예: 2 epoch\n",
    "    bs=64,                   # 배치 사이즈\n",
    "    ckpt_dir=checkpoint_dir\n",
    ")\n",
    "print(\"[Step 10] Fine-tuning finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495002d4-6897-4479-9317-880122decbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# (11) 최종 평가 (원하면 CSV 생성)\n",
    "######################################################\n",
    "print(\"[Step 11] Testing after fine-tuning...\")\n",
    "all_labels_ft, all_preds_ft = test_model(model, test_loader)\n",
    "\n",
    "# 파인튜닝 후 CSV 저장\n",
    "submission_ft = pd.read_csv('./sample_submission.csv')\n",
    "submission_ft['Label'] = all_preds_ft\n",
    "submission_ft.to_csv('./submission_epochFineTune.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_epochFineTune.csv'.\")\n",
    "print(\"----- Done -----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1_pytorch_2.5.1",
   "language": "python",
   "name": "1_pytorch_2.5.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
